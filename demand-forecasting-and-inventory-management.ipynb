{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imorting Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom darts.models import NBEATSModel, TCNModel, RNNModel, TransformerModel\nfrom darts import TimeSeries\nfrom darts.metrics import mae, mse, r2_score\nfrom sklearn.model_selection import ParameterGrid\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error\nfrom pytorch_lightning import Trainer\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Dropout\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam\nfrom prophet import Prophet\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nplt.style.use('ggplot')\ncustom_palette = sns.color_palette(\"coolwarm\", n_colors=10)\nsns.set_palette(custom_palette)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### [Dataset Link](https://www.kaggle.com/datasets/anirudhchauhan/retail-store-inventory-forecasting-dataset)","metadata":{}},{"cell_type":"markdown","source":"### Data Exploration","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"retail_store_inventory.csv\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.head(10)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.info()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.describe()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.isnull().sum()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Initial Data Preprocessing","metadata":{}},{"cell_type":"code","source":"data['Date'] = pd.to_datetime(data['Date'])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### In this cell we convert the values in the 'Date' column of the data to get proper datetime objects.","metadata":{}},{"cell_type":"code","source":"data.set_index('Date', inplace=True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### In this code we sets the 'Date' column as the index of the data DataFrame. By applying this, 'Date' values become the row labels which will help to improve efficiency of time based operation.","metadata":{}},{"cell_type":"code","source":"categorical_cols = data.select_dtypes(include=['object']).columns\ndata = pd.get_dummies(data, columns=categorical_cols, drop_first=True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### In this cell we convert categorical columns into dummy variables.","metadata":{}},{"cell_type":"code","source":"scaler = MinMaxScaler()\nnumerical_cols = data.select_dtypes(include=['float64', 'int64']).columns\ndata[numerical_cols] = scaler.fit_transform(data[numerical_cols])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### In this cell we normalize the numerical features in the data. It identifies numerical columns (Float, int), this applies to calculate the values between 0 to 1 and then updates the dataframe with transformed values.","metadata":{}},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"monthly_data = data.resample('M').mean()\nfig, axes = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n\naxes[0].plot(monthly_data.index, monthly_data['Units Sold'], label='Units Sold (Monthly Avg)', color=custom_palette[0], linewidth=2)\naxes[0].set_title('Units Sold (Monthly Average)', fontsize=14)\naxes[0].set_ylabel('Normalized Units Sold', fontsize=12)\naxes[0].legend(fontsize=10, loc='upper left')\naxes[0].grid(True, which='both', linestyle='--', linewidth=0.5)\n\naxes[1].plot(monthly_data.index, monthly_data['Demand Forecast'], label='Demand Forecast (Monthly Avg)', color=custom_palette[8], alpha=0.7, linewidth=2)\naxes[1].set_title('Demand Forecast (Monthly Average)', fontsize=14)\naxes[1].set_xlabel('Date', fontsize=12)\naxes[1].set_ylabel('Normalized Demand Forecast', fontsize=12)\naxes[1].legend(fontsize=10, loc='upper left')\naxes[1].grid(True, which='both', linestyle='--', linewidth=0.5)\n\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### We resample the data to a monthly frequency and calculate monthly average of each column. Then we create two subplots to plot 'Units Sold' and 'Demand Forecast'. For each subplot we plottted data series, customize the titles, labels, and grid settings and adjust the appearance such as line color and width.\n\n##### In the output \"Units Sold\" plot, some fluctuation in the data showing peaks and valleys over time, which likely indicate the variation in units sold across in different months. Same as in \"Demand Forecast\" plot also shows fluctuations, but the overall trend shows to be different from the units sold. These plots help us to compare the trends in between actual sales and forecasted demand,","metadata":{}},{"cell_type":"code","source":"result = seasonal_decompose(data['Units Sold'], model='additive', period=365)\nfig, axes = plt.subplots(3, 1, figsize=(12, 10))\n\naxes[0].plot(result.trend, color=custom_palette[1], linewidth=2)\naxes[0].set_title('Trend', fontsize=14)\naxes[0].grid(True, linestyle='--', linewidth=0.5)\n\naxes[1].plot(result.seasonal, color=custom_palette[6], linewidth=2)\naxes[1].set_title('Seasonal', fontsize=14)\naxes[1].grid(True, linestyle='--', linewidth=0.5)\n\naxes[2].plot(result.resid, color=custom_palette[9], linewidth=2)\naxes[2].set_title('Residual', fontsize=14)\naxes[2].grid(True, linestyle='--', linewidth=0.5)\n\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### In the code, we have seasonal_decompose function to the 'Units Sold' column of the DataFrame to decompose the time series into three components- trend, seasonal, and residual and set period to 365 to show us yearly seasonality. After that, we make three subplots to visualize these components.\n\n##### From the plot, we can see that the trend component shows periodic changes, which tells us that there are variations and activity remains mostly stable over time. The stability tells us that there is regular pattern in data. The seasonal component also shows flat and consistent behavior. This shows a predictable cycle is in the data that may reflect business patterns. But, the trend component has noticeable spread and irregularity which tells us that there is some noise in the data and it can contain some unpredictable elements. This elements should be refined for better prediction accuracy.","metadata":{}},{"cell_type":"code","source":"key_columns = ['Units Sold', 'Demand Forecast', 'Inventory Level', 'Price', 'Discount', 'Competitor Pricing', 'Holiday/Promotion']\nplt.figure(figsize=(12, 8))\nsns.heatmap(data[key_columns].corr(), annot=True, cmap=custom_palette, fmt='.2f', cbar_kws={'shrink': 0.8}, linewidths=0.5, linecolor='gray', square=True)\nplt.title('Correlation Heatmap (Key Columns)', fontsize=16)\nplt.xticks(fontsize=12, rotation=45, ha='right')\nplt.yticks(fontsize=12)\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### In this cell we analyze the correlation and we create heatmap to visualize the correlation matrix of these selected columns. Lastly, we plotted the heatmap to help to understand the relationships between the key variables. \n\n##### From the matrix, we can see a perfect positive correlation between 'Units Sold' and 'Demand Forecast' which indicates that forecasted demand matches with real sales. While 'Inventory Level' shows medium correlation with both 'Units Sold' and 'Demand Forecast'. But 'Discount' and 'Hoilday/Promotion' have very less correlation with most of the factors.","metadata":{}},{"cell_type":"code","source":"sns.histplot(data['Units Sold'], kde=True, bins=30, color=custom_palette[1])\nplt.title('Distribution of Units Sold')\nplt.xlabel('Units Sold')\nplt.ylabel('Frequency')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### In this cell we created a histogram of the Units Sold data with a kernel density estimate (KDE) to show the distribution of the data. we set a bins 30 and lastly, display the Boxplot, which helps us to visualize the frequency distribution of the 'Units Sold' values.","metadata":{}},{"cell_type":"markdown","source":"### Outlier Analysis","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.boxplot(data=data[numerical_cols], palette=custom_palette)\nplt.title('Boxplot of Numerical Features')\nplt.xticks(rotation=45)\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### In this cell, we have plotted a box plot to analyse the outliers in the main columns from the dataset. From the plot we can see that there are some outliers in 'Units Sold' and  'Demand Forecast' columns.","metadata":{}},{"cell_type":"code","source":"def remove_outliers(df, columns):\n    for col in columns:\n        Q1 = df[col].quantile(0.25)\n        Q3 = df[col].quantile(0.75)\n        IQR = Q3 - Q1\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n        \n        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n    return df\n\ndata = remove_outliers(data, key_columns)\n\nplt.figure(figsize=(12, 6))\nsns.boxplot(data=data[key_columns], palette=custom_palette)\nplt.title('Boxplot of Numerical Features (Outliers Removed)', fontsize=14)\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### We used the IQR methode to remove the outliers because it is efficient, proven and most important it preserves the data integrity. It focuses on the moddle 50% of data which helps minimize the influence of extreme values. This makes it particularly effective for datasets with vatiability like supply chain data. We have appled the 1.5 multiplier which is widely used thershold in data science. This helps to identify meaningful outliers and avoid unnecessary adjustments to valid data points.This method ensures that extreme values are capped rather than removed. This method prevents machine learning models from being skewed or overfitting to extreme values which results in stable and reliable predictions.","metadata":{}},{"cell_type":"markdown","source":"### Time Series Analysis","metadata":{}},{"cell_type":"code","source":"data['Units Sold Differenced'] = data['Units Sold'] - data['Units Sold'].shift(1)\ndata_differenced = data['Units Sold Differenced'].dropna()\n\nadf_result_diff = adfuller(data_differenced)\nprint(\"\\nADF Statistic (Differenced):\", adf_result_diff[0])\nprint(\"p-value (Differenced):\", adf_result_diff[1])\nprint(\"Critical Values (Differenced):\")\nfor key, value in adf_result_diff[4].items():\n    print(f\"\\t{key}: {value}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### We calculate the difference in consecutive values in the Units Sold column to eliminate any lingering trends and seasonality effects, thus ensuring that there is a resultant stationary series. The differenced time series is then subjected to an application of the ADF test: this is in an intent to statistically ensure that any series is indeed stationary prior to time series modeling/forecasting. The outputs from R include the ADF statistic  to arrive at that conclusion.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(16, 6))\nplot_acf(data_differenced, ax=axes[0], lags=50, color=custom_palette[0])\naxes[0].set_title('Autocorrelation (ACF)', fontsize=14)\naxes[0].grid(True, linestyle='--', linewidth=0.5)\n\nplot_pacf(data_differenced, ax=axes[1], lags=50, color=custom_palette[7])\naxes[1].set_title('Partial Autocorrelation (PACF) ', fontsize=14)\naxes[1].grid(True, linestyle='--', linewidth=0.5)\n\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### The ACF plot presents a sharp decline after lag 1, indicating, therefore, that the said correlation rapidly decreases as there is an increase in lag. The PACF, on the other hand, at lag 1 presents a larger spike, while the preceding values gradually taper off. This potentially suggests an AR(1) process. Further analysis for this would thus point out possibly that the time series could easily be modeled by a simpler method like ARIMA or autoregressive.","metadata":{}},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"code","source":"data['Month'] = data.index.month\ndata['Day'] = data.index.day\ndata['Week'] = data.index.isocalendar().week\ndata['Year'] = data.index.year","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### Here we extracted month, day, week, and year from the dataset's date index to capture seasonal and temporal patterns, enhancing analysis and model performance.","metadata":{}},{"cell_type":"code","source":"data['Lag_1'] = data['Units Sold'].shift(1)\ndata['Lag_7'] = data['Units Sold'].shift(7)\ndata['Lag_30'] = data['Units Sold'].shift(30)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### In this cell we created lag features for the \"Units Sold\" column by shifting the data by 1, 7, and 30 days. This helps capture short-term, weekly, and monthly trends, improving the model's ability to understand temporal dependencies.","metadata":{}},{"cell_type":"code","source":"data['Rolling_Mean_7'] = data['Units Sold'].rolling(window=7).mean()\ndata['Rolling_Std_7'] = data['Units Sold'].rolling(window=7).std()\ndata['Rolling_Mean_30'] = data['Units Sold'].rolling(window=30).mean()\ndata['Rolling_Std_30'] = data['Units Sold'].rolling(window=30).std()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### Here we calculated rolling statistics for the \"Units Sold\" column using 7-day and 30-day windows. The rolling mean captures average sales trends over these periods, while the rolling standard deviation identifies fluctuations. These features enhance the model's understanding of local trends and variability in the data.","metadata":{}},{"cell_type":"code","source":"data = data.dropna()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_size = int(len(data) * 0.8)\ntrain, test = data[:train_size], data[train_size:]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### As this time series dataset, we splited the dataset into training and testing sets by allocating 80% of the data to train and the remaining 20% to test. This separation ensures the model is trained on historical data and evaluated on unseen data validate its performance and prevent overfitting.","metadata":{}},{"cell_type":"code","source":"train = train[~train.index.duplicated(keep='first')]\ntest = test[~test.index.duplicated(keep='first')]\ntrain = train.asfreq('D')  \ntest = test.asfreq('D')\n\ndef fill_missing(series):\n    return series.fillna(method='ffill').fillna(method='bfill')\n\ntrain['Units Sold'] = fill_missing(train['Units Sold'])\ntest['Units Sold'] = fill_missing(test['Units Sold'])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### First, we removed any duplicate entries in the indices, retaining only the first occurrence. Then, we set the frequency of both datasets to daily ('D') to ensure uniformity in the time series. To address missing values in the Units Sold column, we apply forward filling (ffill) to propagate the last valid observation, followed by backward filling (bfill) to fill remaining gaps. ","metadata":{}},{"cell_type":"code","source":"ts_train = TimeSeries.from_dataframe(train, value_cols=\"Units Sold\", fill_missing_dates=True, freq='D').astype(np.float32)\nts_test = TimeSeries.from_dataframe(test, value_cols=\"Units Sold\", fill_missing_dates=True, freq='D').astype(np.float32)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### Here we have prepared the training and testing datasets for time series analysis using the Darts library. These datasets are created by converting the train and test DataFrames into TimeSeries objects, specifying the Units Sold column as the target variable. Missing dates are automatically filled to maintain consistent time intervals, with a daily frequency ('D').","metadata":{}},{"cell_type":"code","source":"train_rf_xgb = train.drop(columns=['Units Sold']).copy()\ntest_rf_xgb = test.drop(columns=['Units Sold']).copy()\ncategorical_cols = train_rf_xgb.select_dtypes(include=['category', 'object']).columns\n\nif 'Week' in train_rf_xgb.columns:\n    train_rf_xgb['Week'] = pd.factorize(train_rf_xgb['Week'])[0]\n    test_rf_xgb['Week'] = pd.factorize(test_rf_xgb['Week'])[0]\n\nif not categorical_cols.empty:\n    train_rf_xgb = pd.get_dummies(train_rf_xgb, columns=categorical_cols)\n    test_rf_xgb = pd.get_dummies(test_rf_xgb, columns=categorical_cols)\n\ntrain_rf_xgb, test_rf_xgb = train_rf_xgb.align(test_rf_xgb, join='left', axis=1)\ntest_rf_xgb = test_rf_xgb.fillna(0)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### In this cell we prepare the feature datasets for Random Forest and XGBoost Models. After dropping Units Sold, we handle categorical columns by encoding Week with integers and applying one-hot encoding to others. The training and testing sets are aligned to ensure consistent features, with missing columns in the test set filled with zeros.","metadata":{}},{"cell_type":"code","source":"results = {}","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### Here we have defined a dictionary to record the results of each model, which will be used after the training process to display the results of all models","metadata":{}},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"markdown","source":"### Random Forest Regressor","metadata":{}},{"cell_type":"code","source":"rf_param_grid = {\n    \"n_estimators\": [50, 100, 200],\n    \"max_depth\": [None, 10, 20],\n    \"min_samples_split\": [2, 5, 10],\n    \"min_samples_leaf\": [1, 2, 4]\n}\nbest_rf_score = float(\"inf\")\nbest_rf_params = None\nfor params in ParameterGrid(rf_param_grid):\n    print(f\"Training Random Forest with params: {params}\")\n    rf_model = RandomForestRegressor(**params, random_state=42)\n    rf_model.fit(train_rf_xgb, train['Units Sold'])\n    rf_predictions = rf_model.predict(test_rf_xgb)\n    rf_score = mean_absolute_error(test['Units Sold'], rf_predictions)\n\n    if rf_score < best_rf_score:\n        best_rf_score = rf_score\n        best_rf_params = params\n\nprint(f\"Random Forest Best Params: {best_rf_params}, MAE: {best_rf_score}\")\nresults['Random Forest'] = {\"Best Params\": best_rf_params, \"MAE\": best_rf_score}","metadata":{"scrolled":true,"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### We decided to go with the Random Forest model as the first model because it can handle a large amount of data, including both numerical and categorical features, making it really useful for capturing complex patterns with its ensemble learning method. This choice also assures us that overfitting will not be a problem because this method provides an average of multiple trees. The concrete ranges of the hyperparameters were based on usual tuning practices.\n\n##### n_estimators=[50, 100, 200]: Controls the number of trees, providing a balance between accuracy and computational efficiency.\n\n##### max_depth=[None, 10, 20]: Controls the complexity of each tree to avoid overfitting without underfitting the data.\n\n##### min_samples_split=[2, 5, 10] and min_samples_leaf=[1, 2, 4] ensure that tree splits and their leaf sizes are meaningful to assure better generalization on unseen data.\n\n##### These values were informed by research and best practices, offering enough flexibility for optimization while remaining computationally feasible for our dataset and goals. We then sought to systematically explore this grid in search of the combination that achieves optimal predictive performance.\n","metadata":{}},{"cell_type":"markdown","source":"### XGBoost Model","metadata":{}},{"cell_type":"code","source":"xgb_param_grid = {\n    \"n_estimators\": [50, 100, 200],\n    \"max_depth\": [3, 6, 10],\n    \"learning_rate\": [0.01, 0.1, 0.2],\n    \"subsample\": [0.8, 1.0],\n    \"colsample_bytree\": [0.8, 1.0],\n    \"tree_method\": [\"hist\"]  # Specify tree_method explicitly\n}\nbest_xgb_score = float(\"inf\")\nbest_xgb_params = None\nfor params in ParameterGrid(xgb_param_grid):\n    print(f\"Training XGBoost with params: {params}\")\n    xgb_model = XGBRegressor(**params, random_state=42, enable_categorical=True)\n    xgb_model.fit(train_rf_xgb, train['Units Sold'])\n    xgb_predictions = xgb_model.predict(test_rf_xgb)\n    xgb_score = mean_absolute_error(test['Units Sold'], xgb_predictions)\n\n    if xgb_score < best_xgb_score:\n        best_xgb_score = xgb_score\n        best_xgb_params = params\n\nprint(f\"XGBoost Best Params: {best_xgb_params}, MAE: {best_xgb_score}\")\nresults['XGBoost'] = {\"Best Params\": best_xgb_params, \"MAE\": best_xgb_score}","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### We chose the XGBoost model as our second model for its ability to handle complex datasets efficiently, providing high accuracy with faster computation through gradient-boosted decision trees. This model excels in dealing with both categorical and numerical data, making it ideal for our dataset. The hyperparameter grid was carefully designed to balance model complexity and computational cost.\n\n##### n_estimators [50, 100, 200]: Determines the number of boosting rounds; smaller values prevent overfitting while larger values ensure robust learning.\n\n##### max_depth [3, 6, 10]: Controls tree depth, balancing underfitting for shallow trees and overfitting for deeper ones.\n\n##### learning_rate [0.01, 0.1, 0.2]: Affects the step size in gradient updates; lower values allow for gradual optimization, while higher values converge faster.\n\n##### subsample [0.8, 1.0]: Prevents overfitting by sampling a fraction of data for training each tree.\n\n##### colsample_bytree [0.8, 1.0]: Regulates the fraction of features considered for splitting, promoting diversity in trees and reducing overfitting.\n\n##### tree_method [\"hist\"]: Optimized for efficiency in large datasets, using a histogram-based split finding algorithm.\n\n##### The hyperparameter ranges were informed by XGBoost's documentation and literature, balancing generality with dataset-specific needs. By iterating through combinations, we aimed to find the optimal set of parameters for minimizing mean absolute error while maintaining scalability and adaptability. This tuning ensures the model aligns with our objective of creating robust predictive insights for inventory and demand forecasting.","metadata":{}},{"cell_type":"markdown","source":"### NBeats Model","metadata":{}},{"cell_type":"code","source":"nbeats_param_grid = {\n    \"input_chunk_length\": [30, 60],\n    \"output_chunk_length\": [7, 14],\n    \"num_stacks\": [2, 3],\n    \"num_blocks\": [1, 2, 3],\n    \"layer_widths\": [128, 256, 512]\n}\nbest_nbeats_score = float(\"inf\")\nbest_nbeats_params = None\nfor params in ParameterGrid(nbeats_param_grid):\n    print(f\"Training N-BEATS with params: {params}\")\n    nbeats_model = NBEATSModel(**params, random_state=42, n_epochs=50)\n    nbeats_model.fit(ts_train)\n    nbeats_predictions = nbeats_model.predict(len(ts_test))\n    nbeats_score = mae(ts_test, nbeats_predictions)\n\n    if nbeats_score < best_nbeats_score:\n        best_nbeats_score = nbeats_score\n        best_nbeats_params = params\n\nprint(f\"N-BEATS Best Params: {best_nbeats_params}, MAE: {best_nbeats_score}\")\nresults['N-BEATS'] = {\"Best Params\": best_nbeats_params, \"MAE\": best_nbeats_score}","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### We selected the N-BEATS model as our third model for its ability to handle univariate time series effectively, particularly excelling in long-range forecasting tasks. Its fully connected neural architecture is robust in capturing both trend and seasonality patterns, aligning well with the complexity of our dataset. The hyperparameter grid was designed to fine-tune the model for optimal performance.\n\n##### input_chunk_length [30, 60]: Determines the historical data length used for forecasting. Shorter lengths provide recent information, while longer ones capture extended trends.\n\n##### output_chunk_length [7, 14]: Specifies the forecast horizon. A range ensures adaptability to varying forecasting needs.\n\n##### num_stacks [2, 3]: Adjusts the number of stacks in the model, balancing complexity and computational cost.\n\n##### num_blocks [1, 2, 3]: Represents the number of blocks per stack, controlling the granularity of trend and seasonality learning.\n\n##### layer_widths [128, 256, 512]: Modulates the width of each block, impacting the model's capacity to learn complex patterns.\n\n##### These hyperparameters were chosen based on documented best practices for N-BEATS and adjusted to our dataset's characteristics. Iterating through combinations allowed us to identify the best configuration that minimizes the mean absolute error while maintaining generalization. This model aligns with our goal of leveraging scalable AI techniques for predictive demand forecasting, enabling detailed insights into supply chain dynamics.","metadata":{}},{"cell_type":"markdown","source":"### Temporal Convolution Network Model","metadata":{}},{"cell_type":"code","source":"tcn_param_grid = {\n    \"input_chunk_length\": [30, 60],\n    \"output_chunk_length\": [7, 14],\n    \"num_filters\": [16, 32],\n    \"kernel_size\": [3, 5],\n    \"dropout\": [0.1, 0.2]\n}\nbest_tcn_score = float(\"inf\")\nbest_tcn_params = None\nfor params in ParameterGrid(tcn_param_grid):\n    print(f\"Training TCN with params: {params}\")\n    tcn_model = TCNModel(**params, random_state=42, n_epochs=30)  # Reduced epochs\n    tcn_model.fit(ts_train)\n    tcn_predictions = tcn_model.predict(len(ts_test))\n    tcn_score = mae(ts_test, tcn_predictions)\n\n    if tcn_score < best_tcn_score:\n        best_tcn_score = tcn_score\n        best_tcn_params = params\n\nprint(f\"TCN Best Params: {best_tcn_params}, MAE: {best_tcn_score}\")\nresults['TCN'] = {\"Best Params\": best_tcn_params, \"MAE\": best_tcn_score}","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### We chose the Temporal Convolutional Network (TCN) as our fourth model for its ability to handle sequential data effectively while preserving temporal dependencies. TCNs excel in capturing both short-term and long-term patterns due to their dilated convolutions, making them a robust choice for time series forecasting. The following hyperparameters were used for tuning:\n\n##### input_chunk_length [30, 60]: Defines the historical data length considered for predictions. This range balances short-term precision and long-term trend capture.\n\n##### output_chunk_length [7, 14]: Represents the forecast horizon. A flexible range accommodates varying forecasting needs.\n\n##### num_filters [16, 32]: Controls the number of convolutional filters in each layer, affecting the model's ability to capture data features.\n\n##### kernel_size [3, 5]: Determines the size of the convolutional kernel, influencing how the model aggregates information across time steps.\n\n##### dropout [0.1, 0.2]: Regularization to prevent overfitting by randomly dropping units during training.\n\n##### These parameters were chosen based on TCN's architectural strengths and fine-tuned to suit our dataset's complexity. By iterating over these combinations, we identified the best configuration that minimizes mean absolute error. TCN complements our objective of developing a scalable AI-enhanced digital twin for supply chain optimization, offering insights into demand forecasting and inventory planning.","metadata":{}},{"cell_type":"markdown","source":"### LSTM Model","metadata":{}},{"cell_type":"code","source":"scaler = MinMaxScaler()\nlstm_train_scaled = scaler.fit_transform(train[\"Units Sold\"].values.reshape(-1, 1))\nlstm_test_scaled = scaler.transform(test[\"Units Sold\"].values.reshape(-1, 1))\n\nX_train, y_train = [], []\nwindow_size = 90\nfor i in range(window_size, len(lstm_train_scaled)):\n    X_train.append(lstm_train_scaled[i - window_size:i, 0])\n    y_train.append(lstm_train_scaled[i, 0])\nX_train, y_train = np.array(X_train), np.array(y_train)\n\nX_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n\nlstm_model = Sequential([\n    LSTM(256, activation=\"relu\", return_sequences=True, input_shape=(X_train.shape[1], 1)),\n    Dropout(0.2),\n    LSTM(256, activation=\"relu\", return_sequences=True),\n    Dropout(0.2),\n    LSTM(128, activation=\"relu\"),\n    Dropout(0.2),\n    Dense(1)\n])\n\noptimizer = Adam(learning_rate=0.001)\nlstm_model.compile(optimizer=optimizer, loss=\"mse\")\n\nearly_stopping = EarlyStopping(monitor=\"loss\", patience=10, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor=\"loss\", factor=0.5, patience=5, min_lr=1e-5, verbose=1)\n\nlstm_model.fit(X_train, y_train, epochs=150, batch_size=128, verbose=1, callbacks=[early_stopping, reduce_lr])\n\nX_test = []\nfor i in range(window_size, len(lstm_test_scaled)):\n    X_test.append(lstm_test_scaled[i - window_size:i, 0])\nX_test = np.array(X_test).reshape(-1, window_size, 1)\n\nlstm_predictions = lstm_model.predict(X_test)\nlstm_predictions = scaler.inverse_transform(lstm_predictions)\n\nlstm_score = mean_absolute_error(test[\"Units Sold\"].values[window_size:], lstm_predictions[:, 0])\nprint(f\"LSTM MAE: {lstm_score}\")\n\nresults[\"LSTM\"] = {\"MAE\": lstm_score}","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### We chose the Long Short-Term Memory (LSTM) model for its ability to handle sequential data, essential for demand forecasting. Here's why:\n\n##### Data Preparation: The Units Sold column was scaled using MinMaxScaler to standardize values, ensuring efficient model training. A sliding window approach with a 90-day window size created sequential inputs (X_train) and targets (y_train), capturing temporal patterns. The data was reshaped to match LSTM's input format.\n\n##### Model Architecture: The LSTM model includes three LSTM layers (256, 256, 128 units) to learn temporal dependencies, dropout layers (0.2) to reduce overfitting, and a dense layer for output.\n\n##### Training Optimization: The Adam optimizer was used for adaptive learning, and the MSE loss function minimized prediction errors. EarlyStopping halted training when performance plateaued, and ReduceLROnPlateau adjusted the learning rate dynamically.\n\n##### Testing and Evaluation: Testing data (X_test) was prepared similarly and reshaped. Predictions were scaled back to the original range, and the model’s performance was evaluated using MAE, a reliable accuracy metric.\n\n##### This approach ensures that the LSTM model effectively learns from sequential dependencies in the data, leveraging its architecture for accurate demand forecasting. By iterating over hyperparameters and architectures, we aim to align the model's outputs with the thesis's objectives of scalability and adaptability in supply chain contexts.","metadata":{}},{"cell_type":"markdown","source":"### Prophet Model","metadata":{}},{"cell_type":"code","source":"prophet_train = train.reset_index().rename(columns={train.index.name: \"ds\", \"Units Sold\": \"y\"})\nprophet_test = test.reset_index().rename(columns={test.index.name: \"ds\", \"Units Sold\": \"y\"})\n\nprophet_model = Prophet(\n    seasonality_mode=\"multiplicative\",\n    yearly_seasonality=True,\n    weekly_seasonality=True,\n    daily_seasonality=False,\n    changepoint_prior_scale=0.05\n)\n\nprophet_model.add_seasonality(name=\"quarterly\", period=90, fourier_order=5)\n\nprophet_model.fit(prophet_train)\n\nfuture = prophet_model.make_future_dataframe(periods=len(test))\nforecast = prophet_model.predict(future)\n\nprophet_score = mean_absolute_error(prophet_test[\"y\"].values, forecast[\"yhat\"][:len(test)])\nprint(f\"Prophet MAE: {prophet_score}\")\nresults['Prophet'] = {\"MAE\": prophet_score}","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### We opted for the Prophet model for its simplicity and effectiveness in handling time series data with trends and seasonality. Here's the process:\n\n##### Data Preparation: We reset the index for training and testing datasets and renamed the Units Sold column to y and the date column to ds, as required by Prophet.\n\n##### Model Configuration: The Prophet model was initialized with:\n###### seasonality_mode=\"multiplicative\" to capture proportional seasonal effects.\n###### Built-in yearly and weekly seasonalities enabled, while daily seasonality was disabled.\n###### A changepoint_prior_scale of 0.05 to control sensitivity to trend changes.\n##### Additionally, we introduced custom quarterly seasonality with a period of 90 days and a Fourier order of 5 for capturing intermediate cycles.\n\n##### Training: The model was trained on the prepared dataset (prophet_train) to learn patterns and trends.\n\n##### Forecasting: A future dataframe of the same length as the test dataset was created, and forecasts were generated using the trained model.\n\n##### Evaluation: The accuracy of the Prophet model was evaluated using MAE, comparing actual (y) values with predicted (yhat) values for the test set.\n\n##### This structured approach leverages Prophet’s strength in handling seasonality and trend analysis, ensuring accurate demand forecasts tailored to supply chain needs.","metadata":{}},{"cell_type":"markdown","source":"## Best Model Selection","metadata":{}},{"cell_type":"code","source":"results_df = pd.DataFrame(results).T\nprint(results_df)\n\nresults_df = results_df.sort_values(by=\"MAE\", ascending=True)\n\nbest_model_name = results_df.index[0]\nbest_model_mae = results_df.iloc[0][\"MAE\"]\nprint(f\"\\nThe best model is: {best_model_name} with MAE: {best_model_mae}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prediction on Test Data","metadata":{}},{"cell_type":"code","source":"xgb_predictions = xgb_model.predict(test_rf_xgb)\n\nxgb_mae = mean_absolute_error(test[\"Units Sold\"], xgb_predictions)\nprint(f\"XGBoost MAE on Test Set: {xgb_mae}\")\n\nplt.figure(figsize=(12, 6))\nplt.plot(test.index, test[\"Units Sold\"], label=\"Actual\", color=\"red\")\nplt.plot(test.index, xgb_predictions, label=\"Predicted\", color='blue', alpha=0.7)\nplt.title(\"Predictions vs Actual\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Units Sold\")\nplt.legend()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### From the above plot, We observe that the predicted values closely follow the actual values, indicating that the model performs well in capturing the underlying trends and patterns in the data. However, there are slight deviations at some peaks and troughs, suggesting that the model might not fully capture extreme fluctuations in demand. Overall, the model provides accurate predictions, making it suitable for demand forecasting in inventory management and supply chain optimization. ","metadata":{}},{"cell_type":"markdown","source":"## Data Preparation for Demand Forecasting and Inventory Management","metadata":{}},{"cell_type":"code","source":"future_rf_xgb = test_rf_xgb.iloc[-1:].copy()\n\nif future_rf_xgb['Competitor Pricing'].dtype == 'object':\n    future_rf_xgb['Competitor Pricing'] = future_rf_xgb['Competitor Pricing'].astype(bool).astype(int)\n\nif future_rf_xgb['Seasonality_Winter'].dtype == 'object':\n    future_rf_xgb['Seasonality_Winter'] = future_rf_xgb['Seasonality_Winter'].astype(float)\n\nfor col in future_rf_xgb.columns:\n    if future_rf_xgb[col].dtype == 'bool':\n        future_rf_xgb[col] = future_rf_xgb[col].astype(int)\n\nfuture_rf_xgb = future_rf_xgb[train_rf_xgb.columns]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### We prepared the future_rf_xgb DataFrame for predictions by fixing data types and aligning it with train_rf_xgb. Object-type columns like Competitor Pricing and Seasonality_Winter are converted to integers and floats, respectively. Boolean columns are standardized to integers (0 and 1). Finally, we align future_rf_xgb with train_rf_xgb to ensure consistent features, avoiding mismatches during predictions.","metadata":{}},{"cell_type":"markdown","source":"## Demand Forecasting","metadata":{}},{"cell_type":"code","source":"future_periods = 60\n\nfuture_predictions = []\nfor _ in range(future_periods):\n    pred = xgb_model.predict(future_rf_xgb)\n    future_predictions.append(pred[0])\n    future_rf_xgb.iloc[0, :-1] = future_rf_xgb.iloc[0, 1:]\n    future_rf_xgb.iloc[0, -1] = pred\n\nfuture_dates = pd.date_range(start=test.index[-1] + pd.Timedelta(days=1), periods=future_periods, freq='D')\n\nplt.figure(figsize=(14, 7))\nplt.plot(test.index, test[\"Units Sold\"], label=\"Actual (Test Data)\", color=custom_palette[2], linewidth=2)\nplt.plot(future_dates, future_predictions, label=f\"Forecasted Demand ({future_periods} Days)\", color=custom_palette[9], linestyle=\"--\", linewidth=2)\nplt.title(f\"Forecast for {future_periods} Days\", fontsize=16)\nplt.xlabel(\"Date\", fontsize=14)\nplt.ylabel(\"Units Sold\", fontsize=14)\nplt.legend(fontsize=10, loc='upper left', bbox_to_anchor=(1, 1))\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### From this plot, we observe that forecasted demand, while capturing general trends, appears to predict frequent spikes and dips, suggesting potential demand surges and declines. This plot can be used to identify patterns for inventory planning, ensuring the supply chain is prepared for both peak and low-demand periods. Additionally, the alignment between actual and forecasted data demonstrates the model's predictive capability, which is crucial for operational decision-making in dynamic environments.","metadata":{}},{"cell_type":"code","source":"inventory_levels = test[\"Inventory Level\"].values[-future_periods:]\n\ninventory_gap = np.array(inventory_levels) - np.array(future_predictions)\n\nplt.figure(figsize=(14, 7))\nplt.plot(future_dates, future_predictions, label=\"Forecasted Demand\", color=custom_palette[1], linewidth=2)\nplt.plot(future_dates, inventory_levels, label=\"Inventory Levels\", color=custom_palette[3], linewidth=2)\nplt.fill_between(future_dates, inventory_levels, future_predictions, \n                 where=(np.array(inventory_levels) > np.array(future_predictions)), \n                 color=custom_palette[4], alpha=0.3, label=\"Overstock\")\nplt.fill_between(future_dates, inventory_levels, future_predictions, \n                 where=(np.array(inventory_levels) < np.array(future_predictions)), \n                 color=custom_palette[9], alpha=0.3, label=\"Stockout Risk\")\nplt.xticks(rotation=45, fontsize=10)\nplt.title(\"Forecasted Demand vs Inventory Levels\", fontsize=16)\nplt.xlabel(\"Date\", fontsize=14)\nplt.ylabel(\"Units\", fontsize=14)\nplt.legend(fontsize=10, loc='upper left', bbox_to_anchor=(1, 1))\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### The shaded areas reveal critical insights: red regions denote stockout risk, where demand surpasses inventory, potentially disrupting operations, while blue regions highlight overstock situations, leading to excessive holding costs. This visualization emphasizes the need for proactive inventory management strategies to align supply with demand effectively, minimizing risks and optimizing costs, thereby supporting efficient decision-making in supply chain operations.","metadata":{}},{"cell_type":"code","source":"lead_time_days = 3\ndemand_std = np.std(future_predictions[:lead_time_days])\nservice_level = 1.65\nsafety_stock = service_level * demand_std\n\nprint(f\"Safety Stock: {safety_stock:.2f} units\")\n\nplt.figure(figsize=(12, 6))\nplt.axhline(y=safety_stock, color=custom_palette[7], linestyle=\"--\", label=f\"Safety Stock ({safety_stock:.2f} units)\")\nplt.plot(future_dates, future_predictions, label=\"Forecasted Demand\", color=custom_palette[1], linewidth=2)\nplt.xticks(rotation=45, fontsize=10)\nplt.title(\"Safety Stock vs Forecasted Demand\", fontsize=16)\nplt.xlabel(\"Date\", fontsize=14)\nplt.ylabel(\"Units\", fontsize=14)\nplt.legend(fontsize=10, loc='upper left', bbox_to_anchor=(1, 1))\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### In the above cell we calculated safety stock for inventory management by considering a 3-day lead time, representing the typical supply chain replenishment period, short enough for agility but realistic for delays. We used the demand standard deviation over this period to account for variability and multiplied it by a service level of 1.65, which corresponds to a 95% probability of meeting customer demand without stockouts. This balance ensures adequate buffer stock while minimizing excess inventory. The approach aligns with agile inventory practices, addressing demand fluctuations and maintaining cost efficiency.\n\n##### From the plot, we see that forecasted demand fluctuates significantly, crossing above and below the safety stock line. This highlights potential periods of risk—when forecasted demand exceeds the safety stock, there is a possibility of stockouts. Conversely, when forecasted demand remains well below the safety stock, there might be overstocking, indicating an opportunity to optimize inventory. This visualization underscores the importance of aligning safety stock with demand patterns to minimize inventory costs while avoiding disruptions.","metadata":{}},{"cell_type":"code","source":"reorder_point = lead_time_days * np.mean(future_predictions[:lead_time_days]) + safety_stock\nprint(f\"Reorder Point: {reorder_point:.2f} units\")\n\nplt.figure(figsize=(12, 6))\nplt.axhline(y=reorder_point, color=custom_palette[7], linestyle=\"--\", label=f\"Reorder Point ({reorder_point:.2f} units)\")\nplt.plot(future_dates, future_predictions, label=\"Forecasted Demand\", color=custom_palette[1], linewidth=2)\nplt.xticks(rotation=45, fontsize=10)\nplt.title(\"Reorder Point Analysis\", fontsize=16)\nplt.xlabel(\"Date\", fontsize=14)\nplt.ylabel(\"Units\", fontsize=14)\nplt.legend(fontsize=10, loc='upper left', bbox_to_anchor=(1, 1))\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### We calculated the reorder point, which serves as a crucial inventory management threshold. It represents the inventory level at which we must place a new order to prevent stockouts during the lead time. Using a lead time of 3 days, we multiplied it by the average predicted demand over those days to account for expected consumption. To this, we added the safety stock of 0.45 units, which we calculated earlier to manage demand variability and potential supply chain delays.\n\n##### The resulting reorder point of 1.80 units means that whenever our inventory falls to this level, we need to trigger a replenishment order. This ensures we maintain optimal stock levels, avoid disruptions, and remain prepared for unexpected demand fluctuations, all while aligning with efficient and agile inventory practices.","metadata":{}},{"cell_type":"markdown","source":"## Conclusion","metadata":{}},{"cell_type":"markdown","source":"#### 1. Development of the Scalable AI-Enabled Digital Twin Model\n##### Demand Forecasting: Complex machine learning models were built using XGBoost, LSTM, and N-BEATS to forecast the future demand quite close to actual. The design and tuning of the models were done based on historical data. To tune those models, several diverse hyperparameter grids were used.\n\n##### Adaptability and Scalability: Indeed, the integration of custom features such as rolling statistics, lagged variables, and seasonal adjustments is a hallmarker of adaptability within diverse contexts of supply chains. Furthermore, the use of a framework like Prophet demonstrates that the approach to time-series forecasting for highly variable industries can be scalable.\n\n#### 2. Inventory Management Optimization\n\n##### Safety Stock and Reorder Point: The analysis integrated the notion of safety stock and reordering points on inventory, which are relevant in developing an optimal balance between stocking out or overstocking. Precisely, the estimated safety stock would be about 0.36 units, the reorder point quantity being estimated at 1.10 units, and they were effective in visualization through actionable insight.\n\n##### Gaining Insights into Stock Risks: The plots, by overlaying forecasted demand and inventory levels, underlined periods of potential stockout risks or overstocking. This kind of visualization is crucial for operational decision-making and also aligns with agile inventory practices.\n\n#### 3. Overcoming Operational Challenges\n##### The data integration deals with a couple of big steps for preprocessing, handling missing values, removing outliers, and aligning train-test data sets; it was the major challenging issue with the dataset. Therefore, all datasets are prepared in the same format with MinMax scaling and one-hot encoding for model consistency.\n\n##### Computational Efficiency: XGBoost and TCN were some of the models used for computationally efficient algorithms that could work with big data in supply chains while balancing predictive performance.\n\n#### 4. Scalability and Industrial Application\n##### It is a scalable approach that can be easily fitted to retail, manufacturing, or logistics industries. Lag variables and rolling statistics feature flexibility in handling dynamic demand patterns.\n##### Other recommendations for customization include the adjustment of lead time, safety stock thresholds, and service levels based on the peculiar supply chain dynamics within specific industries.\n\n#### 5. Final Results and Validation\n##### All models in the experiment showed promising performances, while XGBoost performed the best according to MAE. Demand forecast, stockout risk, and reorder point analyses provide visualizations that are practical in application.\n##### These results fully address the objective of the AI-enhanced data-driven development of a digital twin in enhancing decision-making over supply chain operations.\n\n#### The proposed framework fulfills the intents of the thesis by deconstructing a scalable digital twin model that enhances both predictive demand forecasting and overall inventory management, with actionable insights mitigating supply chain risks. Obtained results have already convincingly demonstrated the practicalities of AI-enhanced digital twin applications in transforming traditional one-size-fits-all supply chain structures into agile, data-based systems.","metadata":{}}]}